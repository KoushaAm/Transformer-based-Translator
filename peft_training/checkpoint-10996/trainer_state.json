{
  "best_global_step": 10996,
  "best_metric": 3.6127495765686035,
  "best_model_checkpoint": "peft_training/checkpoint-10996",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 10996,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0181909136386375,
      "grad_norm": 603.4442749023438,
      "learning_rate": 3.559322033898305e-05,
      "loss": 27.8519,
      "step": 50
    },
    {
      "epoch": 0.036381827277275,
      "grad_norm": 843.3950805664062,
      "learning_rate": 7.191283292978207e-05,
      "loss": 25.1262,
      "step": 100
    },
    {
      "epoch": 0.0545727409159125,
      "grad_norm": 142.52731323242188,
      "learning_rate": 0.0001082324455205811,
      "loss": 19.6418,
      "step": 150
    },
    {
      "epoch": 0.07276365455455,
      "grad_norm": 233.98268127441406,
      "learning_rate": 0.00014455205811138012,
      "loss": 14.0521,
      "step": 200
    },
    {
      "epoch": 0.09095456819318751,
      "grad_norm": 2.8776919841766357,
      "learning_rate": 0.00018087167070217916,
      "loss": 10.1471,
      "step": 250
    },
    {
      "epoch": 0.109145481831825,
      "grad_norm": 1.621954083442688,
      "learning_rate": 0.0002171912832929782,
      "loss": 8.7082,
      "step": 300
    },
    {
      "epoch": 0.1273363954704625,
      "grad_norm": 1.6239262819290161,
      "learning_rate": 0.00025351089588377724,
      "loss": 8.1291,
      "step": 350
    },
    {
      "epoch": 0.1455273091091,
      "grad_norm": 19.187469482421875,
      "learning_rate": 0.00028983050847457623,
      "loss": 7.7063,
      "step": 400
    },
    {
      "epoch": 0.1637182227477375,
      "grad_norm": 6.722864627838135,
      "learning_rate": 0.00029999460274675793,
      "loss": 6.7539,
      "step": 450
    },
    {
      "epoch": 0.18190913638637501,
      "grad_norm": 1.9306527376174927,
      "learning_rate": 0.00029996919987790104,
      "loss": 6.3258,
      "step": 500
    },
    {
      "epoch": 0.2001000500250125,
      "grad_norm": 1.628463864326477,
      "learning_rate": 0.0002999229786643747,
      "loss": 6.0115,
      "step": 550
    },
    {
      "epoch": 0.21829096366365,
      "grad_norm": 1.4009193181991577,
      "learning_rate": 0.00029985594552249073,
      "loss": 5.8114,
      "step": 600
    },
    {
      "epoch": 0.2364818773022875,
      "grad_norm": 1.662855863571167,
      "learning_rate": 0.00029976810975762015,
      "loss": 5.7646,
      "step": 650
    },
    {
      "epoch": 0.254672790940925,
      "grad_norm": 1.4674723148345947,
      "learning_rate": 0.00029965948356290155,
      "loss": 5.7176,
      "step": 700
    },
    {
      "epoch": 0.2728637045795625,
      "grad_norm": 1.464056372642517,
      "learning_rate": 0.00029953008201754814,
      "loss": 5.6148,
      "step": 750
    },
    {
      "epoch": 0.2910546182182,
      "grad_norm": 2.578796148300171,
      "learning_rate": 0.000299379923084755,
      "loss": 5.6205,
      "step": 800
    },
    {
      "epoch": 0.3092455318568375,
      "grad_norm": 1.3233906030654907,
      "learning_rate": 0.000299209027609205,
      "loss": 5.5547,
      "step": 850
    },
    {
      "epoch": 0.327436445495475,
      "grad_norm": 1.3277580738067627,
      "learning_rate": 0.00029901741931417545,
      "loss": 5.5253,
      "step": 900
    },
    {
      "epoch": 0.3456273591341125,
      "grad_norm": 1.433086633682251,
      "learning_rate": 0.00029880512479824473,
      "loss": 5.4921,
      "step": 950
    },
    {
      "epoch": 0.36381827277275003,
      "grad_norm": 1.366377830505371,
      "learning_rate": 0.0002985721735316001,
      "loss": 5.4675,
      "step": 1000
    },
    {
      "epoch": 0.38200918641138754,
      "grad_norm": 1.4092386960983276,
      "learning_rate": 0.00029831859785194677,
      "loss": 5.4123,
      "step": 1050
    },
    {
      "epoch": 0.400200100050025,
      "grad_norm": 1.4683494567871094,
      "learning_rate": 0.0002980444329600185,
      "loss": 5.3443,
      "step": 1100
    },
    {
      "epoch": 0.4183910136886625,
      "grad_norm": 1.5647412538528442,
      "learning_rate": 0.00029774971691469166,
      "loss": 5.3641,
      "step": 1150
    },
    {
      "epoch": 0.4365819273273,
      "grad_norm": 1.6047492027282715,
      "learning_rate": 0.0002974344906277015,
      "loss": 5.3072,
      "step": 1200
    },
    {
      "epoch": 0.4547728409659375,
      "grad_norm": 1.5602339506149292,
      "learning_rate": 0.0002970987978579631,
      "loss": 5.2049,
      "step": 1250
    },
    {
      "epoch": 0.472963754604575,
      "grad_norm": 1.415571689605713,
      "learning_rate": 0.00029674268520549706,
      "loss": 5.1938,
      "step": 1300
    },
    {
      "epoch": 0.4911546682432125,
      "grad_norm": 1.6091009378433228,
      "learning_rate": 0.00029636620210496016,
      "loss": 5.2418,
      "step": 1350
    },
    {
      "epoch": 0.50934558188185,
      "grad_norm": 1.997389554977417,
      "learning_rate": 0.00029596940081878337,
      "loss": 5.1866,
      "step": 1400
    },
    {
      "epoch": 0.5275364955204875,
      "grad_norm": 1.761012315750122,
      "learning_rate": 0.0002955523364299167,
      "loss": 5.1691,
      "step": 1450
    },
    {
      "epoch": 0.545727409159125,
      "grad_norm": 1.403884768486023,
      "learning_rate": 0.0002951150668341828,
      "loss": 5.1656,
      "step": 1500
    },
    {
      "epoch": 0.5639183227977626,
      "grad_norm": 1.2608616352081299,
      "learning_rate": 0.00029465765273224,
      "loss": 5.1847,
      "step": 1550
    },
    {
      "epoch": 0.5821092364364,
      "grad_norm": 2.2323784828186035,
      "learning_rate": 0.0002941801576211558,
      "loss": 5.1286,
      "step": 1600
    },
    {
      "epoch": 0.6003001500750376,
      "grad_norm": 1.5761785507202148,
      "learning_rate": 0.00029368264778559293,
      "loss": 5.1432,
      "step": 1650
    },
    {
      "epoch": 0.618491063713675,
      "grad_norm": 1.6187145709991455,
      "learning_rate": 0.0002931651922886071,
      "loss": 5.1046,
      "step": 1700
    },
    {
      "epoch": 0.6366819773523125,
      "grad_norm": 1.9625062942504883,
      "learning_rate": 0.00029262786296206054,
      "loss": 5.145,
      "step": 1750
    },
    {
      "epoch": 0.65487289099095,
      "grad_norm": 1.5073010921478271,
      "learning_rate": 0.00029207073439665,
      "loss": 5.0861,
      "step": 1800
    },
    {
      "epoch": 0.6730638046295875,
      "grad_norm": 1.6208363771438599,
      "learning_rate": 0.00029149388393155256,
      "loss": 5.108,
      "step": 1850
    },
    {
      "epoch": 0.691254718268225,
      "grad_norm": 1.488956332206726,
      "learning_rate": 0.00029089739164368946,
      "loss": 5.0723,
      "step": 1900
    },
    {
      "epoch": 0.7094456319068625,
      "grad_norm": 1.4912883043289185,
      "learning_rate": 0.00029028134033661,
      "loss": 5.015,
      "step": 1950
    },
    {
      "epoch": 0.7276365455455001,
      "grad_norm": 1.854384183883667,
      "learning_rate": 0.0002896458155289971,
      "loss": 5.0652,
      "step": 2000
    },
    {
      "epoch": 0.7458274591841375,
      "grad_norm": 1.5548152923583984,
      "learning_rate": 0.00028899090544279565,
      "loss": 5.0332,
      "step": 2050
    },
    {
      "epoch": 0.7640183728227751,
      "grad_norm": 1.631562352180481,
      "learning_rate": 0.00028831670099096584,
      "loss": 5.0036,
      "step": 2100
    },
    {
      "epoch": 0.7822092864614125,
      "grad_norm": 1.539351463317871,
      "learning_rate": 0.0002876232957648629,
      "loss": 5.0288,
      "step": 2150
    },
    {
      "epoch": 0.80040020010005,
      "grad_norm": 1.6043298244476318,
      "learning_rate": 0.0002869107860212449,
      "loss": 4.9288,
      "step": 2200
    },
    {
      "epoch": 0.8185911137386875,
      "grad_norm": 1.4829589128494263,
      "learning_rate": 0.00028617927066891075,
      "loss": 5.0353,
      "step": 2250
    },
    {
      "epoch": 0.836782027377325,
      "grad_norm": 1.8118200302124023,
      "learning_rate": 0.0002854288512549697,
      "loss": 4.9541,
      "step": 2300
    },
    {
      "epoch": 0.8549729410159626,
      "grad_norm": 1.7113683223724365,
      "learning_rate": 0.0002846596319507451,
      "loss": 4.9485,
      "step": 2350
    },
    {
      "epoch": 0.8731638546546,
      "grad_norm": 1.7312140464782715,
      "learning_rate": 0.00028387171953731335,
      "loss": 4.934,
      "step": 2400
    },
    {
      "epoch": 0.8913547682932376,
      "grad_norm": 1.4937421083450317,
      "learning_rate": 0.0002830652233906811,
      "loss": 4.8725,
      "step": 2450
    },
    {
      "epoch": 0.909545681931875,
      "grad_norm": 1.5956168174743652,
      "learning_rate": 0.00028224025546660177,
      "loss": 4.9405,
      "step": 2500
    },
    {
      "epoch": 0.9277365955705126,
      "grad_norm": 1.680815577507019,
      "learning_rate": 0.00028139693028503396,
      "loss": 4.9789,
      "step": 2550
    },
    {
      "epoch": 0.94592750920915,
      "grad_norm": 1.8400840759277344,
      "learning_rate": 0.0002805353649142446,
      "loss": 4.9094,
      "step": 2600
    },
    {
      "epoch": 0.9641184228477875,
      "grad_norm": 1.553478479385376,
      "learning_rate": 0.00027965567895455725,
      "loss": 4.9101,
      "step": 2650
    },
    {
      "epoch": 0.982309336486425,
      "grad_norm": 1.5955060720443726,
      "learning_rate": 0.0002787579945217499,
      "loss": 4.8669,
      "step": 2700
    },
    {
      "epoch": 1.0,
      "eval_loss": 4.15105676651001,
      "eval_runtime": 32.3416,
      "eval_samples_per_second": 339.934,
      "eval_steps_per_second": 42.515,
      "step": 2749
    },
    {
      "epoch": 1.0003638182727728,
      "grad_norm": 1.878237009048462,
      "learning_rate": 0.0002778424362301029,
      "loss": 4.8727,
      "step": 2750
    },
    {
      "epoch": 1.0185547319114103,
      "grad_norm": 1.6533640623092651,
      "learning_rate": 0.0002769091311751006,
      "loss": 4.8217,
      "step": 2800
    },
    {
      "epoch": 1.0367456455500477,
      "grad_norm": 1.5199073553085327,
      "learning_rate": 0.00027595820891578797,
      "loss": 4.7939,
      "step": 2850
    },
    {
      "epoch": 1.0549365591886852,
      "grad_norm": 1.4994257688522339,
      "learning_rate": 0.0002749898014567856,
      "loss": 4.9025,
      "step": 2900
    },
    {
      "epoch": 1.0731274728273228,
      "grad_norm": 1.329383134841919,
      "learning_rate": 0.00027400404322996556,
      "loss": 4.819,
      "step": 2950
    },
    {
      "epoch": 1.0913183864659604,
      "grad_norm": 1.7400964498519897,
      "learning_rate": 0.0002730010710757892,
      "loss": 4.8382,
      "step": 3000
    },
    {
      "epoch": 1.1095093001045977,
      "grad_norm": 1.707742691040039,
      "learning_rate": 0.000271981024224312,
      "loss": 4.8416,
      "step": 3050
    },
    {
      "epoch": 1.1277002137432353,
      "grad_norm": 1.55733323097229,
      "learning_rate": 0.00027094404427585546,
      "loss": 4.7965,
      "step": 3100
    },
    {
      "epoch": 1.1458911273818728,
      "grad_norm": 1.6004970073699951,
      "learning_rate": 0.00026989027518135086,
      "loss": 4.8415,
      "step": 3150
    },
    {
      "epoch": 1.1640820410205102,
      "grad_norm": 1.7817497253417969,
      "learning_rate": 0.0002688198632223563,
      "loss": 4.8572,
      "step": 3200
    },
    {
      "epoch": 1.1822729546591477,
      "grad_norm": 1.7040070295333862,
      "learning_rate": 0.00026773295699075,
      "loss": 4.7882,
      "step": 3250
    },
    {
      "epoch": 1.2004638682977853,
      "grad_norm": 1.8575359582901,
      "learning_rate": 0.0002666297073681035,
      "loss": 4.8125,
      "step": 3300
    },
    {
      "epoch": 1.2186547819364228,
      "grad_norm": 1.7823456525802612,
      "learning_rate": 0.0002655102675047364,
      "loss": 4.7742,
      "step": 3350
    },
    {
      "epoch": 1.2368456955750602,
      "grad_norm": 1.3347294330596924,
      "learning_rate": 0.00026437479279845666,
      "loss": 4.7871,
      "step": 3400
    },
    {
      "epoch": 1.2550366092136978,
      "grad_norm": 1.6914124488830566,
      "learning_rate": 0.00026322344087298854,
      "loss": 4.7972,
      "step": 3450
    },
    {
      "epoch": 1.2732275228523353,
      "grad_norm": 1.7513269186019897,
      "learning_rate": 0.00026205637155609144,
      "loss": 4.7952,
      "step": 3500
    },
    {
      "epoch": 1.2914184364909729,
      "grad_norm": 2.0101890563964844,
      "learning_rate": 0.00026087374685737353,
      "loss": 4.7832,
      "step": 3550
    },
    {
      "epoch": 1.3096093501296102,
      "grad_norm": 1.8031355142593384,
      "learning_rate": 0.00025967573094580156,
      "loss": 4.7051,
      "step": 3600
    },
    {
      "epoch": 1.3278002637682478,
      "grad_norm": 1.8034080266952515,
      "learning_rate": 0.00025846249012691143,
      "loss": 4.7682,
      "step": 3650
    },
    {
      "epoch": 1.3459911774068853,
      "grad_norm": 1.589098334312439,
      "learning_rate": 0.0002572341928197224,
      "loss": 4.7706,
      "step": 3700
    },
    {
      "epoch": 1.3641820910455227,
      "grad_norm": 2.0134544372558594,
      "learning_rate": 0.0002559910095333569,
      "loss": 4.7353,
      "step": 3750
    },
    {
      "epoch": 1.3823730046841602,
      "grad_norm": 1.5996830463409424,
      "learning_rate": 0.0002547331128433718,
      "loss": 4.722,
      "step": 3800
    },
    {
      "epoch": 1.4005639183227978,
      "grad_norm": 1.5767149925231934,
      "learning_rate": 0.0002534606773678009,
      "loss": 4.7383,
      "step": 3850
    },
    {
      "epoch": 1.4187548319614351,
      "grad_norm": 1.6743830442428589,
      "learning_rate": 0.0002521738797429158,
      "loss": 4.6923,
      "step": 3900
    },
    {
      "epoch": 1.4369457456000727,
      "grad_norm": 1.71595299243927,
      "learning_rate": 0.0002508728985987051,
      "loss": 4.7144,
      "step": 3950
    },
    {
      "epoch": 1.4551366592387103,
      "grad_norm": 1.836077332496643,
      "learning_rate": 0.00024955791453407783,
      "loss": 4.6393,
      "step": 4000
    },
    {
      "epoch": 1.4733275728773478,
      "grad_norm": 1.803546667098999,
      "learning_rate": 0.00024822911009179276,
      "loss": 4.6814,
      "step": 4050
    },
    {
      "epoch": 1.4915184865159854,
      "grad_norm": 1.7132679224014282,
      "learning_rate": 0.00024688666973311866,
      "loss": 4.683,
      "step": 4100
    },
    {
      "epoch": 1.5097094001546227,
      "grad_norm": 2.1894195079803467,
      "learning_rate": 0.00024553077981222753,
      "loss": 4.7238,
      "step": 4150
    },
    {
      "epoch": 1.5279003137932603,
      "grad_norm": 1.6193722486495972,
      "learning_rate": 0.0002441616285503258,
      "loss": 4.7534,
      "step": 4200
    },
    {
      "epoch": 1.5460912274318979,
      "grad_norm": 1.9228256940841675,
      "learning_rate": 0.00024277940600952547,
      "loss": 4.6313,
      "step": 4250
    },
    {
      "epoch": 1.5642821410705352,
      "grad_norm": 1.748435139656067,
      "learning_rate": 0.00024138430406646046,
      "loss": 4.6363,
      "step": 4300
    },
    {
      "epoch": 1.5824730547091728,
      "grad_norm": 1.8754620552062988,
      "learning_rate": 0.00023997651638565075,
      "loss": 4.7041,
      "step": 4350
    },
    {
      "epoch": 1.6006639683478103,
      "grad_norm": 1.6612930297851562,
      "learning_rate": 0.00023855623839261824,
      "loss": 4.623,
      "step": 4400
    },
    {
      "epoch": 1.6188548819864477,
      "grad_norm": 1.6635499000549316,
      "learning_rate": 0.00023712366724675844,
      "loss": 4.6841,
      "step": 4450
    },
    {
      "epoch": 1.6370457956250852,
      "grad_norm": 2.0320072174072266,
      "learning_rate": 0.00023567900181397107,
      "loss": 4.6393,
      "step": 4500
    },
    {
      "epoch": 1.6552367092637228,
      "grad_norm": 2.2665462493896484,
      "learning_rate": 0.0002342224426390543,
      "loss": 4.6939,
      "step": 4550
    },
    {
      "epoch": 1.6734276229023601,
      "grad_norm": 1.7465877532958984,
      "learning_rate": 0.00023275419191786542,
      "loss": 4.6001,
      "step": 4600
    },
    {
      "epoch": 1.691618536540998,
      "grad_norm": 1.67167067527771,
      "learning_rate": 0.00023127445346925253,
      "loss": 4.6167,
      "step": 4650
    },
    {
      "epoch": 1.7098094501796353,
      "grad_norm": 1.850952386856079,
      "learning_rate": 0.00022978343270676103,
      "loss": 4.6182,
      "step": 4700
    },
    {
      "epoch": 1.7280003638182728,
      "grad_norm": 2.173851490020752,
      "learning_rate": 0.00022828133661011857,
      "loss": 4.5826,
      "step": 4750
    },
    {
      "epoch": 1.7461912774569104,
      "grad_norm": 1.7226593494415283,
      "learning_rate": 0.00022676837369650258,
      "loss": 4.6146,
      "step": 4800
    },
    {
      "epoch": 1.7643821910955477,
      "grad_norm": 1.5095428228378296,
      "learning_rate": 0.00022524475399159454,
      "loss": 4.6348,
      "step": 4850
    },
    {
      "epoch": 1.7825731047341853,
      "grad_norm": 1.9157172441482544,
      "learning_rate": 0.00022371068900042476,
      "loss": 4.5728,
      "step": 4900
    },
    {
      "epoch": 1.8007640183728229,
      "grad_norm": 2.1383142471313477,
      "learning_rate": 0.00022216639167801176,
      "loss": 4.6255,
      "step": 4950
    },
    {
      "epoch": 1.8189549320114602,
      "grad_norm": 1.5888291597366333,
      "learning_rate": 0.00022061207639980027,
      "loss": 4.6018,
      "step": 5000
    },
    {
      "epoch": 1.8371458456500978,
      "grad_norm": 1.8553502559661865,
      "learning_rate": 0.00021904795893190244,
      "loss": 4.5658,
      "step": 5050
    },
    {
      "epoch": 1.8553367592887353,
      "grad_norm": 1.7158703804016113,
      "learning_rate": 0.00021747425640114538,
      "loss": 4.571,
      "step": 5100
    },
    {
      "epoch": 1.8735276729273727,
      "grad_norm": 1.7063219547271729,
      "learning_rate": 0.00021589118726493052,
      "loss": 4.6337,
      "step": 5150
    },
    {
      "epoch": 1.8917185865660104,
      "grad_norm": 2.0085482597351074,
      "learning_rate": 0.00021429897128090775,
      "loss": 4.6135,
      "step": 5200
    },
    {
      "epoch": 1.9099095002046478,
      "grad_norm": 1.802046298980713,
      "learning_rate": 0.00021269782947646912,
      "loss": 4.5544,
      "step": 5250
    },
    {
      "epoch": 1.9281004138432851,
      "grad_norm": 1.8915071487426758,
      "learning_rate": 0.00021108798411806655,
      "loss": 4.5682,
      "step": 5300
    },
    {
      "epoch": 1.946291327481923,
      "grad_norm": 1.5646616220474243,
      "learning_rate": 0.0002094696586803572,
      "loss": 4.5936,
      "step": 5350
    },
    {
      "epoch": 1.9644822411205602,
      "grad_norm": 1.6688836812973022,
      "learning_rate": 0.0002078430778151814,
      "loss": 4.5552,
      "step": 5400
    },
    {
      "epoch": 1.9826731547591978,
      "grad_norm": 1.988468050956726,
      "learning_rate": 0.00020620846732037718,
      "loss": 4.56,
      "step": 5450
    },
    {
      "epoch": 2.0,
      "eval_loss": 3.822645902633667,
      "eval_runtime": 32.8897,
      "eval_samples_per_second": 334.268,
      "eval_steps_per_second": 41.806,
      "step": 5498
    },
    {
      "epoch": 2.0007276365455455,
      "grad_norm": 1.8873316049575806,
      "learning_rate": 0.0002045660541084352,
      "loss": 4.5687,
      "step": 5500
    },
    {
      "epoch": 2.018918550184183,
      "grad_norm": 1.6355984210968018,
      "learning_rate": 0.00020291606617499968,
      "loss": 4.5101,
      "step": 5550
    },
    {
      "epoch": 2.0371094638228207,
      "grad_norm": 1.923435091972351,
      "learning_rate": 0.00020125873256721848,
      "loss": 4.5719,
      "step": 5600
    },
    {
      "epoch": 2.055300377461458,
      "grad_norm": 1.651275634765625,
      "learning_rate": 0.0001995942833519474,
      "loss": 4.4779,
      "step": 5650
    },
    {
      "epoch": 2.0734912911000953,
      "grad_norm": 2.067310333251953,
      "learning_rate": 0.0001979229495838128,
      "loss": 4.5127,
      "step": 5700
    },
    {
      "epoch": 2.091682204738733,
      "grad_norm": 2.1033103466033936,
      "learning_rate": 0.0001962449632731373,
      "loss": 4.5228,
      "step": 5750
    },
    {
      "epoch": 2.1098731183773705,
      "grad_norm": 1.7210475206375122,
      "learning_rate": 0.00019456055735373256,
      "loss": 4.52,
      "step": 5800
    },
    {
      "epoch": 2.128064032016008,
      "grad_norm": 1.742983341217041,
      "learning_rate": 0.00019286996565056415,
      "loss": 4.537,
      "step": 5850
    },
    {
      "epoch": 2.1462549456546456,
      "grad_norm": 1.8974523544311523,
      "learning_rate": 0.00019117342284729252,
      "loss": 4.5418,
      "step": 5900
    },
    {
      "epoch": 2.164445859293283,
      "grad_norm": 1.7830209732055664,
      "learning_rate": 0.0001894711644536946,
      "loss": 4.5512,
      "step": 5950
    },
    {
      "epoch": 2.1826367729319207,
      "grad_norm": 1.963968276977539,
      "learning_rate": 0.00018776342677297124,
      "loss": 4.4656,
      "step": 6000
    },
    {
      "epoch": 2.200827686570558,
      "grad_norm": 1.8478918075561523,
      "learning_rate": 0.00018605044686894403,
      "loss": 4.4864,
      "step": 6050
    },
    {
      "epoch": 2.2190186002091954,
      "grad_norm": 2.351808547973633,
      "learning_rate": 0.00018433246253314665,
      "loss": 4.5166,
      "step": 6100
    },
    {
      "epoch": 2.237209513847833,
      "grad_norm": 1.8985153436660767,
      "learning_rate": 0.00018260971225181555,
      "loss": 4.5746,
      "step": 6150
    },
    {
      "epoch": 2.2554004274864705,
      "grad_norm": 2.125056743621826,
      "learning_rate": 0.0001808824351727834,
      "loss": 4.4704,
      "step": 6200
    },
    {
      "epoch": 2.273591341125108,
      "grad_norm": 1.7214688062667847,
      "learning_rate": 0.0001791508710722816,
      "loss": 4.5062,
      "step": 6250
    },
    {
      "epoch": 2.2917822547637456,
      "grad_norm": 1.6893057823181152,
      "learning_rate": 0.000177415260321655,
      "loss": 4.5255,
      "step": 6300
    },
    {
      "epoch": 2.309973168402383,
      "grad_norm": 1.878858208656311,
      "learning_rate": 0.00017567584385399422,
      "loss": 4.5233,
      "step": 6350
    },
    {
      "epoch": 2.3281640820410203,
      "grad_norm": 1.79514741897583,
      "learning_rate": 0.00017393286313068987,
      "loss": 4.4842,
      "step": 6400
    },
    {
      "epoch": 2.346354995679658,
      "grad_norm": 2.018805980682373,
      "learning_rate": 0.00017218656010791343,
      "loss": 4.5256,
      "step": 6450
    },
    {
      "epoch": 2.3645459093182954,
      "grad_norm": 1.5970263481140137,
      "learning_rate": 0.00017043717720302963,
      "loss": 4.4965,
      "step": 6500
    },
    {
      "epoch": 2.3827368229569332,
      "grad_norm": 1.8694161176681519,
      "learning_rate": 0.0001686849572609446,
      "loss": 4.4555,
      "step": 6550
    },
    {
      "epoch": 2.4009277365955706,
      "grad_norm": 1.6824076175689697,
      "learning_rate": 0.00016693014352039495,
      "loss": 4.5187,
      "step": 6600
    },
    {
      "epoch": 2.419118650234208,
      "grad_norm": 1.680273175239563,
      "learning_rate": 0.00016517297958018164,
      "loss": 4.5142,
      "step": 6650
    },
    {
      "epoch": 2.4373095638728457,
      "grad_norm": 2.024414300918579,
      "learning_rate": 0.0001634137093653545,
      "loss": 4.433,
      "step": 6700
    },
    {
      "epoch": 2.455500477511483,
      "grad_norm": 2.174795150756836,
      "learning_rate": 0.00016165257709335095,
      "loss": 4.453,
      "step": 6750
    },
    {
      "epoch": 2.4736913911501204,
      "grad_norm": 2.2980363368988037,
      "learning_rate": 0.00015988982724009462,
      "loss": 4.4516,
      "step": 6800
    },
    {
      "epoch": 2.491882304788758,
      "grad_norm": 1.920554757118225,
      "learning_rate": 0.00015812570450605764,
      "loss": 4.4552,
      "step": 6850
    },
    {
      "epoch": 2.5100732184273955,
      "grad_norm": 1.682700514793396,
      "learning_rate": 0.00015636045378229194,
      "loss": 4.5027,
      "step": 6900
    },
    {
      "epoch": 2.528264132066033,
      "grad_norm": 1.8100666999816895,
      "learning_rate": 0.00015459432011643407,
      "loss": 4.4327,
      "step": 6950
    },
    {
      "epoch": 2.5464550457046706,
      "grad_norm": 1.966019630432129,
      "learning_rate": 0.00015282754867868847,
      "loss": 4.5064,
      "step": 7000
    },
    {
      "epoch": 2.564645959343308,
      "grad_norm": 1.6895908117294312,
      "learning_rate": 0.00015106038472779347,
      "loss": 4.439,
      "step": 7050
    },
    {
      "epoch": 2.5828368729819458,
      "grad_norm": 2.0830516815185547,
      "learning_rate": 0.000149293073576975,
      "loss": 4.4275,
      "step": 7100
    },
    {
      "epoch": 2.601027786620583,
      "grad_norm": 1.8565558195114136,
      "learning_rate": 0.00014752586055989293,
      "loss": 4.4502,
      "step": 7150
    },
    {
      "epoch": 2.6192187002592204,
      "grad_norm": 1.9781699180603027,
      "learning_rate": 0.00014575899099658447,
      "loss": 4.3919,
      "step": 7200
    },
    {
      "epoch": 2.6374096138978578,
      "grad_norm": 1.9929418563842773,
      "learning_rate": 0.00014399271015940945,
      "loss": 4.4783,
      "step": 7250
    },
    {
      "epoch": 2.6556005275364956,
      "grad_norm": 1.9597827196121216,
      "learning_rate": 0.00014222726323900224,
      "loss": 4.4394,
      "step": 7300
    },
    {
      "epoch": 2.673791441175133,
      "grad_norm": 1.7109473943710327,
      "learning_rate": 0.00014046289531023496,
      "loss": 4.376,
      "step": 7350
    },
    {
      "epoch": 2.6919823548137707,
      "grad_norm": 1.8942241668701172,
      "learning_rate": 0.00013869985129819686,
      "loss": 4.4245,
      "step": 7400
    },
    {
      "epoch": 2.710173268452408,
      "grad_norm": 1.8988040685653687,
      "learning_rate": 0.00013693837594419444,
      "loss": 4.4924,
      "step": 7450
    },
    {
      "epoch": 2.7283641820910454,
      "grad_norm": 2.003469944000244,
      "learning_rate": 0.000135178713771777,
      "loss": 4.3945,
      "step": 7500
    },
    {
      "epoch": 2.746555095729683,
      "grad_norm": 3.0993504524230957,
      "learning_rate": 0.00013342110905279264,
      "loss": 4.4358,
      "step": 7550
    },
    {
      "epoch": 2.7647460093683205,
      "grad_norm": 1.9217485189437866,
      "learning_rate": 0.00013166580577347897,
      "loss": 4.4288,
      "step": 7600
    },
    {
      "epoch": 2.7829369230069583,
      "grad_norm": 1.867922067642212,
      "learning_rate": 0.00012991304760059353,
      "loss": 4.4448,
      "step": 7650
    },
    {
      "epoch": 2.8011278366455956,
      "grad_norm": 2.1270663738250732,
      "learning_rate": 0.0001281630778475886,
      "loss": 4.4176,
      "step": 7700
    },
    {
      "epoch": 2.819318750284233,
      "grad_norm": 1.8112186193466187,
      "learning_rate": 0.00012641613944083517,
      "loss": 4.4056,
      "step": 7750
    },
    {
      "epoch": 2.8375096639228703,
      "grad_norm": 1.6254500150680542,
      "learning_rate": 0.0001246724748859004,
      "loss": 4.4306,
      "step": 7800
    },
    {
      "epoch": 2.855700577561508,
      "grad_norm": 1.8285247087478638,
      "learning_rate": 0.00012293232623388357,
      "loss": 4.4173,
      "step": 7850
    },
    {
      "epoch": 2.8738914912001454,
      "grad_norm": 1.6172206401824951,
      "learning_rate": 0.00012119593504781521,
      "loss": 4.4025,
      "step": 7900
    },
    {
      "epoch": 2.892082404838783,
      "grad_norm": 2.010556221008301,
      "learning_rate": 0.00011946354236912398,
      "loss": 4.3793,
      "step": 7950
    },
    {
      "epoch": 2.9102733184774205,
      "grad_norm": 1.767697811126709,
      "learning_rate": 0.00011773538868417592,
      "loss": 4.4314,
      "step": 8000
    },
    {
      "epoch": 2.928464232116058,
      "grad_norm": 1.924372673034668,
      "learning_rate": 0.00011601171389089057,
      "loss": 4.403,
      "step": 8050
    },
    {
      "epoch": 2.9466551457546957,
      "grad_norm": 1.833327054977417,
      "learning_rate": 0.0001142927572654392,
      "loss": 4.4086,
      "step": 8100
    },
    {
      "epoch": 2.964846059393333,
      "grad_norm": 2.3410425186157227,
      "learning_rate": 0.00011257875742902882,
      "loss": 4.4159,
      "step": 8150
    },
    {
      "epoch": 2.983036973031971,
      "grad_norm": 2.074540138244629,
      "learning_rate": 0.0001108699523147776,
      "loss": 4.4357,
      "step": 8200
    },
    {
      "epoch": 3.0,
      "eval_loss": 3.679394245147705,
      "eval_runtime": 32.2782,
      "eval_samples_per_second": 340.601,
      "eval_steps_per_second": 42.598,
      "step": 8247
    },
    {
      "epoch": 3.0010914548183183,
      "grad_norm": 1.6917743682861328,
      "learning_rate": 0.00010916657913468562,
      "loss": 4.3659,
      "step": 8250
    },
    {
      "epoch": 3.0192823684569556,
      "grad_norm": 1.9812005758285522,
      "learning_rate": 0.0001074688743467055,
      "loss": 4.4037,
      "step": 8300
    },
    {
      "epoch": 3.0374732820955934,
      "grad_norm": 1.8063825368881226,
      "learning_rate": 0.00010577707362191804,
      "loss": 4.3254,
      "step": 8350
    },
    {
      "epoch": 3.0556641957342308,
      "grad_norm": 1.8734080791473389,
      "learning_rate": 0.00010409141181181691,
      "loss": 4.4106,
      "step": 8400
    },
    {
      "epoch": 3.073855109372868,
      "grad_norm": 2.045156717300415,
      "learning_rate": 0.00010241212291570726,
      "loss": 4.3835,
      "step": 8450
    },
    {
      "epoch": 3.092046023011506,
      "grad_norm": 1.9596925973892212,
      "learning_rate": 0.00010073944004822243,
      "loss": 4.3876,
      "step": 8500
    },
    {
      "epoch": 3.1102369366501432,
      "grad_norm": 1.9812002182006836,
      "learning_rate": 9.907359540696352e-05,
      "loss": 4.3523,
      "step": 8550
    },
    {
      "epoch": 3.1284278502887806,
      "grad_norm": 1.8378509283065796,
      "learning_rate": 9.741482024026645e-05,
      "loss": 4.3502,
      "step": 8600
    },
    {
      "epoch": 3.1466187639274183,
      "grad_norm": 1.7685174942016602,
      "learning_rate": 9.576334481510053e-05,
      "loss": 4.3715,
      "step": 8650
    },
    {
      "epoch": 3.1648096775660557,
      "grad_norm": 2.2321059703826904,
      "learning_rate": 9.411939838510352e-05,
      "loss": 4.3942,
      "step": 8700
    },
    {
      "epoch": 3.183000591204693,
      "grad_norm": 1.807214379310608,
      "learning_rate": 9.2483209158757e-05,
      "loss": 4.4311,
      "step": 8750
    },
    {
      "epoch": 3.201191504843331,
      "grad_norm": 1.6675350666046143,
      "learning_rate": 9.08550042677073e-05,
      "loss": 4.3129,
      "step": 8800
    },
    {
      "epoch": 3.219382418481968,
      "grad_norm": 1.9031617641448975,
      "learning_rate": 8.923500973523541e-05,
      "loss": 4.3503,
      "step": 8850
    },
    {
      "epoch": 3.237573332120606,
      "grad_norm": 1.9083744287490845,
      "learning_rate": 8.762345044488124e-05,
      "loss": 4.3507,
      "step": 8900
    },
    {
      "epoch": 3.2557642457592433,
      "grad_norm": 2.0802037715911865,
      "learning_rate": 8.602055010922573e-05,
      "loss": 4.3618,
      "step": 8950
    },
    {
      "epoch": 3.2739551593978806,
      "grad_norm": 1.6836141347885132,
      "learning_rate": 8.442653123883551e-05,
      "loss": 4.3869,
      "step": 9000
    },
    {
      "epoch": 3.2921460730365184,
      "grad_norm": 1.840988039970398,
      "learning_rate": 8.284161511137494e-05,
      "loss": 4.3894,
      "step": 9050
    },
    {
      "epoch": 3.3103369866751557,
      "grad_norm": 1.7702637910842896,
      "learning_rate": 8.126602174088843e-05,
      "loss": 4.2965,
      "step": 9100
    },
    {
      "epoch": 3.328527900313793,
      "grad_norm": 1.9313056468963623,
      "learning_rate": 7.969996984725898e-05,
      "loss": 4.3315,
      "step": 9150
    },
    {
      "epoch": 3.346718813952431,
      "grad_norm": 1.7042878866195679,
      "learning_rate": 7.814367682584587e-05,
      "loss": 4.3649,
      "step": 9200
    },
    {
      "epoch": 3.364909727591068,
      "grad_norm": 1.9023089408874512,
      "learning_rate": 7.659735871730645e-05,
      "loss": 4.4078,
      "step": 9250
    },
    {
      "epoch": 3.3831006412297056,
      "grad_norm": 1.7846052646636963,
      "learning_rate": 7.506123017760597e-05,
      "loss": 4.3878,
      "step": 9300
    },
    {
      "epoch": 3.4012915548683433,
      "grad_norm": 2.04970645904541,
      "learning_rate": 7.353550444821949e-05,
      "loss": 4.3611,
      "step": 9350
    },
    {
      "epoch": 3.4194824685069807,
      "grad_norm": 1.8897345066070557,
      "learning_rate": 7.202039332653023e-05,
      "loss": 4.3663,
      "step": 9400
    },
    {
      "epoch": 3.4376733821456185,
      "grad_norm": 2.45483660697937,
      "learning_rate": 7.051610713642853e-05,
      "loss": 4.3704,
      "step": 9450
    },
    {
      "epoch": 3.455864295784256,
      "grad_norm": 2.061537981033325,
      "learning_rate": 6.902285469911512e-05,
      "loss": 4.3194,
      "step": 9500
    },
    {
      "epoch": 3.474055209422893,
      "grad_norm": 2.050718307495117,
      "learning_rate": 6.754084330411306e-05,
      "loss": 4.3834,
      "step": 9550
    },
    {
      "epoch": 3.492246123061531,
      "grad_norm": 1.988993525505066,
      "learning_rate": 6.607027868049236e-05,
      "loss": 4.3114,
      "step": 9600
    },
    {
      "epoch": 3.5104370367001683,
      "grad_norm": 1.7167731523513794,
      "learning_rate": 6.461136496831126e-05,
      "loss": 4.4028,
      "step": 9650
    },
    {
      "epoch": 3.5286279503388056,
      "grad_norm": 2.131676197052002,
      "learning_rate": 6.316430469027797e-05,
      "loss": 4.3211,
      "step": 9700
    },
    {
      "epoch": 3.5468188639774434,
      "grad_norm": 1.7909892797470093,
      "learning_rate": 6.172929872363688e-05,
      "loss": 4.3643,
      "step": 9750
    },
    {
      "epoch": 3.5650097776160807,
      "grad_norm": 1.8965156078338623,
      "learning_rate": 6.030654627228351e-05,
      "loss": 4.3357,
      "step": 9800
    },
    {
      "epoch": 3.583200691254718,
      "grad_norm": 1.8629204034805298,
      "learning_rate": 5.889624483911136e-05,
      "loss": 4.3847,
      "step": 9850
    },
    {
      "epoch": 3.601391604893356,
      "grad_norm": 1.8272349834442139,
      "learning_rate": 5.749859019859507e-05,
      "loss": 4.3625,
      "step": 9900
    },
    {
      "epoch": 3.619582518531993,
      "grad_norm": 1.890682339668274,
      "learning_rate": 5.611377636961356e-05,
      "loss": 4.3765,
      "step": 9950
    },
    {
      "epoch": 3.637773432170631,
      "grad_norm": 2.045471429824829,
      "learning_rate": 5.474199558851691e-05,
      "loss": 4.3323,
      "step": 10000
    },
    {
      "epoch": 3.6559643458092683,
      "grad_norm": 1.8433655500411987,
      "learning_rate": 5.3383438282440336e-05,
      "loss": 4.3437,
      "step": 10050
    },
    {
      "epoch": 3.6741552594479057,
      "grad_norm": 1.9106757640838623,
      "learning_rate": 5.203829304286978e-05,
      "loss": 4.3093,
      "step": 10100
    },
    {
      "epoch": 3.692346173086543,
      "grad_norm": 2.090527296066284,
      "learning_rate": 5.070674659946208e-05,
      "loss": 4.3059,
      "step": 10150
    },
    {
      "epoch": 3.710537086725181,
      "grad_norm": 1.9608944654464722,
      "learning_rate": 4.93889837941236e-05,
      "loss": 4.3589,
      "step": 10200
    },
    {
      "epoch": 3.728728000363818,
      "grad_norm": 1.7553337812423706,
      "learning_rate": 4.808518755535098e-05,
      "loss": 4.3316,
      "step": 10250
    },
    {
      "epoch": 3.746918914002456,
      "grad_norm": 2.0438990592956543,
      "learning_rate": 4.6795538872837406e-05,
      "loss": 4.3435,
      "step": 10300
    },
    {
      "epoch": 3.7651098276410933,
      "grad_norm": 1.7479732036590576,
      "learning_rate": 4.5520216772348184e-05,
      "loss": 4.2872,
      "step": 10350
    },
    {
      "epoch": 3.7833007412797306,
      "grad_norm": 2.1262073516845703,
      "learning_rate": 4.425939829086866e-05,
      "loss": 4.2736,
      "step": 10400
    },
    {
      "epoch": 3.8014916549183684,
      "grad_norm": 1.9194886684417725,
      "learning_rate": 4.30132584520287e-05,
      "loss": 4.3286,
      "step": 10450
    },
    {
      "epoch": 3.8196825685570057,
      "grad_norm": 1.7765426635742188,
      "learning_rate": 4.178197024180598e-05,
      "loss": 4.3046,
      "step": 10500
    },
    {
      "epoch": 3.8378734821956435,
      "grad_norm": 2.054271697998047,
      "learning_rate": 4.0565704584512745e-05,
      "loss": 4.333,
      "step": 10550
    },
    {
      "epoch": 3.856064395834281,
      "grad_norm": 1.8177694082260132,
      "learning_rate": 3.936463031906849e-05,
      "loss": 4.3209,
      "step": 10600
    },
    {
      "epoch": 3.874255309472918,
      "grad_norm": 1.8691376447677612,
      "learning_rate": 3.817891417556214e-05,
      "loss": 4.3582,
      "step": 10650
    },
    {
      "epoch": 3.8924462231115555,
      "grad_norm": 1.9036771059036255,
      "learning_rate": 3.700872075210702e-05,
      "loss": 4.3245,
      "step": 10700
    },
    {
      "epoch": 3.9106371367501933,
      "grad_norm": 1.8282887935638428,
      "learning_rate": 3.585421249199172e-05,
      "loss": 4.325,
      "step": 10750
    },
    {
      "epoch": 3.9288280503888307,
      "grad_norm": 2.5252931118011475,
      "learning_rate": 3.471554966113023e-05,
      "loss": 4.2756,
      "step": 10800
    },
    {
      "epoch": 3.9470189640274684,
      "grad_norm": 1.8778187036514282,
      "learning_rate": 3.359289032581405e-05,
      "loss": 4.287,
      "step": 10850
    },
    {
      "epoch": 3.9652098776661058,
      "grad_norm": 1.912440299987793,
      "learning_rate": 3.2486390330770114e-05,
      "loss": 4.3494,
      "step": 10900
    },
    {
      "epoch": 3.983400791304743,
      "grad_norm": 2.039830207824707,
      "learning_rate": 3.139620327752651e-05,
      "loss": 4.2574,
      "step": 10950
    },
    {
      "epoch": 4.0,
      "eval_loss": 3.6127495765686035,
      "eval_runtime": 31.6743,
      "eval_samples_per_second": 347.095,
      "eval_steps_per_second": 43.411,
      "step": 10996
    }
  ],
  "logging_steps": 50,
  "max_steps": 13745,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.724882722730803e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
